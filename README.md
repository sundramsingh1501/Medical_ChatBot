ğŸ©º Medical AI Chatbot (RAG-based)

ğŸ”— Live Demo (Hugging Face):
ğŸ‘‰ https://huggingface.co/spaces/sundram1501/medical-ai-chatbot

ğŸ”— GitHub Repository:
ğŸ‘‰ https://github.com/sundramsingh1501/Medical_ChatBot

ğŸ“Œ Overview

Medical AI Chatbot is a Retrieval-Augmented Generation (RAG) based application that answers medical questions strictly from a medical textbook using Pinecone Vector Database and Google Gemini LLM.

Unlike generic chatbots, this system does not hallucinate â€” it retrieves relevant context from indexed medical documents before generating responses.

âš ï¸ Disclaimer: This chatbot is for educational purposes only and is not a substitute for professional medical advice.

ğŸš€ Key Features

ğŸ“š Medical Textbook Grounding (RAG)

ğŸ” Semantic Search using Pinecone

ğŸ¤– Google Gemini (2.5 Flash) LLM

ğŸ§  Sentence Transformers Embeddings

ğŸ’¬ Interactive Streamlit Chat UI

âš¡ Optimized for fast response & lazy loading

â˜ï¸ Deployed on Hugging Face Spaces

ğŸ§  Architecture (How It Works)

Medical textbook PDF is split into chunks

Chunks are converted into vector embeddings

Embeddings are stored in Pinecone

User question â†’ semantic search

Top relevant chunks are retrieved

Gemini LLM answers strictly using retrieved context

User Question
      â†“
Pinecone Vector Search
      â†“
Relevant Medical Context
      â†“
Gemini LLM
      â†“
Final Answer (Context-Grounded)

ğŸ› ï¸ Tech Stack
Layer	Technology
Frontend	Streamlit
LLM	Google Gemini 2.5 Flash
Vector DB	Pinecone
Embeddings	Sentence-Transformers (MiniLM)
Framework	LangChain
Deployment	Hugging Face Spaces
Language	Python
ğŸ“‚ Project Structure
Medical_Chatbot/
â”‚
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ ingest.py        # PDF ingestion into Pinecone
â”‚   â”œâ”€â”€ rag.py           # RAG pipeline logic
â”‚
â”œâ”€â”€ Frontend/
â”‚   â””â”€â”€ app.py           # Streamlit UI
â”‚
â”œâ”€â”€ Data/
â”‚   â””â”€â”€ Medical_book.pdf
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Environment Variables

Create a .env file with:

PINECONE_API_KEY=your_pinecone_key
PINECONE_INDEX_NAME=medical-chatbot
GOOGLE_API_KEY=your_gemini_api_key

â–¶ï¸ Run Locally
1ï¸âƒ£ Create virtual environment
python -m venv venv
venv\Scripts\activate   # Windows

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Ingest medical book (one-time)
python Backend/ingest.py

4ï¸âƒ£ Start Streamlit app
streamlit run Frontend/app.py

â˜ï¸ Deployment

This project is fully deployed on Hugging Face Spaces using a Docker-based Streamlit setup.

Auto-build from GitHub

Secure environment variables

Production-ready inference

ğŸ”— Live App:
ğŸ‘‰ https://huggingface.co/spaces/sundram1501/medical-ai-chatbot

ğŸ¯ Why This Project Matters

Demonstrates real-world GenAI usage

Shows RAG implementation (industry standard)

Prevents hallucinations

Uses modern LLM infrastructure

Suitable for placements, internships, and interviews

ğŸ§‘â€ğŸ’» Author

Kumar Sundram
ğŸ“ B.Tech CSE, IIIT Bhagalpur
ğŸ’¡ AI | ML | GenAI | RAG
ğŸ”— GitHub: https://github.com/sundramsingh1501

â­ Future Improvements

Multi-document ingestion

Streaming responses

Source citation per answer

User chat history persistence

Authentication
